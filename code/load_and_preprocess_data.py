import os
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

def load_data_from_directory(directory, label):
    data = []
    labels = []
    for filename in os.listdir(directory):
        if filename.endswith(".asm"):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                code = file.read()
            data.append(code)
            labels.append(label)
    return data, labels

# Directories containing malware and benign code files
malware_directory = 'train__malware'
benign_directory = 'train__benign'

malware_data, malware_labels = load_data_from_directory(malware_directory, 1)
benign_data, benign_labels = load_data_from_directory(benign_directory, 0)

# Combine data and labels
all_data = malware_data + benign_data
all_labels = malware_labels + benign_labels

# Set the maximum sequence length
max_sequence_length = 1000

# Create a tokenizer and fit it on the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_data)

# Convert text data to sequences
sequences = tokenizer.texts_to_sequences(all_data)

# Pad sequences to ensure uniform input size
data = pad_sequences(sequences, maxlen=max_sequence_length)

# Convert labels to numpy array
labels = np.array(all_labels)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Save the preprocessed data and tokenizer
np.save('X_train.npy', X_train)
np.save('X_test.npy', X_test)
np.save('y_train.npy', y_train)
np.save('y_test.npy', y_test)

with open('tokenizer.pickle', 'wb') as handle:
    import pickle
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
