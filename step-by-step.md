

**Understanding the Data**

disasmbled
├── train_benign/
├── train_malware/
├── data_Set
├── evaluate_model.py
├── load_and_preprocess_data.py
├── preprocess_assembly.py
├── train_model.py

Sure, let's break down the steps for preprocessing the assembly code and then using it to classify malware and benign files using either a CNN (Convolutional Neural Network) or an RNN (Recurrent Neural Network).

## Step 1: Preprocess the Assembly Code

Assuming you have already implemented and executed the preprocessing script provided earlier, which removes comments, extra whitespace, standardizes the code, and labels in your assembly code files.

## Step 2: Prepare the Data

After preprocessing, we need to load the processed files, convert them into a format suitable for machine learning models, and then split them into training and testing sets.

### Load Data and Add Labels

Assuming that malware and benign files are stored in separate directories, we load each file, preprocess it, and label it accordingly.

```python
import os
import numpy as np

def load_data_from_directory(directory, label):
    data = []
    labels = []
    for filename in os.listdir(directory):
        if filename.endswith(".asm"):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                code = file.read()
            data.append(code)
            labels.append(label)
    return data, labels

# Directories containing malware and benign code files
malware_directory = '/path/to/malware/code/files'
benign_directory = '/path/to/benign/code/files'

malware_data, malware_labels = load_data_from_directory(malware_directory, 1)
benign_data, benign_labels = load_data_from_directory(benign_directory, 0)

# Combine data and labels
all_data = malware_data + benign_data
all_labels = malware_labels + benign_labels

```

### Preprocess Text Data and Vectorize

We need to convert the text data into numerical sequences using a tokenizer.

```python
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# Set the maximum sequence length
max_sequence_length = 1000

# Create a tokenizer and fit it on the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_data)

# Convert text data to sequences
sequences = tokenizer.texts_to_sequences(all_data)

# Pad sequences to ensure uniform input size
data = pad_sequences(sequences, maxlen=max_sequence_length)

# Convert labels to numpy array
labels = np.array(all_labels)

```

### Split Data

Split the data into training and testing sets.

```python
from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

```

## Step 3: Define and Train the Model

Define and train either a CNN or an RNN model on the preprocessed data.

### CNN Model Example

```python
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout

# Set the vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# Define the CNN model
cnn_model = Sequential()
cnn_model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))
cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
cnn_model.add(GlobalMaxPooling1D())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(1, activation='sigmoid'))

# Compile the model
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

```

### RNN Model Example

```python
from keras.layers import LSTM

# Define the RNN model
rnn_model = Sequential()
rnn_model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length))
rnn_model.add(LSTM(128, return_sequences=False))
rnn_model.add(Dense(128, activation='relu'))
rnn_model.add(Dropout(0.5))
rnn_model.add(Dense(1, activation='sigmoid'))

# Compile the model
rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

```

## Step 4: Evaluate the Model

After training, evaluate the model's performance on the test set.

```python
# Evaluate the CNN model
cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test)
print(f"CNN Test Accuracy: {cnn_accuracy * 100:.2f}%")

# Evaluate the RNN model
rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test)
print(f"RNN Test Accuracy: {rnn_accuracy * 100:.2f}%")

```

By following these steps, you can preprocess your assembly code files, prepare them for machine learning, and train a CNN or RNN model to classify the files as malware or benign. Adjust the model parameters and preprocessing steps as needed for your specific dataset and requirements.

.

```
import os
import re

# 주석 제거 함수
def remove_comments(code):
    return '\n'.join(line.split(';')[0].strip() for line in code.split('\n') if line.split(';')[0].strip())

# 불필요한 공백 제거 함수
def remove_extra_whitespaces(code):
    return '\n'.join(line.strip() for line in code.split('\n') if line.strip())

# 코드 표준화 함수 (소문자로 변환)
def standardize_code(code):
    return '\n'.join(line.lower() for line in code.split('\n') if line.strip())

# 레이블 표준화 함수
def standardize_labels(code):
    label_pattern = re.compile(r'^\s*(\w+):', re.MULTILINE)
    labels = label_pattern.findall(code)
    label_mapping = {label: f'label_{i}' for i, label in enumerate(labels)}

    def replace_label(match):
        return f'{label_mapping[match.group(1)]}:'

    def replace_label_references(match):
        label = match.group(0)
        return label_mapping.get(label, label)

    # 레이블 정의를 변환
    code = label_pattern.sub(replace_label, code)
    
    # 레이블 참조를 변환
    for label, std_label in label_mapping.items():
        code = re.sub(r'\b' + re.escape(label) + r'\b', std_label, code)
    
    return code

# 전체 전처리 함수
def preprocess_assembly_code(code):
    code = remove_comments(code)
    code = remove_extra_whitespaces(code)
    code = standardize_code(code)
    code = standardize_labels(code)
    return code

# 디렉토리 내의 모든 어셈블리어 파일을 전처리하는 함수
def preprocess_files(directory):
    for filename in os.listdir(directory):
        if filename.endswith(".asm"):
            filepath = os.path.join(directory, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as file:
                    code = file.read()
                processed_code = preprocess_assembly_code(code)
                with open(filepath, 'w', encoding='utf-8') as file:
                    file.write(processed_code)
            except Exception as e:
                print(f"Error processing file {filepath}: {e}")

# 어셈블리어 코드 파일들이 있는 디렉토리 경로 설정
directory_path = 'data_Set'
preprocess_files(directory_path)

```

.

```jsx
import os
import re

# 주석 제거 함수
def remove_comments(code):
    return '\n'.join(line.split(';')[0].strip() for line in code.split('\n') if line.split(';')[0].strip())

# 불필요한 공백 제거 함수
def remove_extra_whitespaces(code):
    return '\n'.join(line.strip() for line in code.split('\n') if line.strip())

# 코드 표준화 함수 (소문자로 변환)
def standardize_code(code):
    return '\n'.join(line.lower() for line in code.split('\n') if line.strip())

# 레이블 표준화 함수
def standardize_labels(code):
    label_pattern = re.compile(r'^\s*(\w+):', re.MULTILINE)
    labels = label_pattern.findall(code)
    label_mapping = {label: f'label_{i}' for i, label in enumerate(labels)}

    def replace_label(match):
        return f'{label_mapping[match.group(1)]}:'

    def replace_label_references(match):
        label = match.group(0)
        return label_mapping.get(label, label)

    # 레이블 정의를 변환
    code = label_pattern.sub(replace_label, code)
    
    # 레이블 참조를 변환
    for label, std_label in label_mapping.items():
        code = re.sub(r'\b' + re.escape(label) + r'\b', std_label, code)
    
    return code

# 전체 전처리 함수
def preprocess_assembly_code(code):
    code = remove_comments(code)
    code = remove_extra_whitespaces(code)
    code = standardize_code(code)
    code = standardize_labels(code)
    return code

# 디렉토리 내의 모든 어셈블리어 파일을 전처리하는 함수
def preprocess_files(directory):
    for root, dirs, files in os.walk(directory):
        for filename in files:
            if filename.endswith(".asm"):
                filepath = os.path.join(root, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as file:
                        code = file.read()
                    processed_code = preprocess_assembly_code(code)
                    with open(filepath, 'w', encoding='utf-8') as file:
                        file.write(processed_code)
                except Exception as e:
                    print(f"Error processing file {filepath}: {e}")

# 어셈블리어 코드 파일들이 있는 디렉토리 경로 설정
directories = ['data_Set1', 'data_Set2', 'data_Set3', 'data_Set4']
for directory_path in directories:
    preprocess_files(directory_path)

```

.

```python
import os
import re

# 주석 제거 함수
def remove_comments(code):
    return '\n'.join(line.split(';')[0].strip() for line in code.split('\n') if line.split(';')[0].strip())

# 불필요한 공백 제거 함수
def remove_extra_whitespaces(code):
    return '\n'.join(line.strip() for line in code.split('\n') if line.strip())

# 코드 표준화 함수 (소문자로 변환)
def standardize_code(code):
    return '\n'.join(line.lower() for line in code.split('\n') if line.strip())

# 레이블 표준화 함수
def standardize_labels(code):
    label_pattern = re.compile(r'^\s*(\w+):', re.MULTILINE)
    labels = label_pattern.findall(code)
    label_mapping = {label: f'label_{i}' for i, label in enumerate(labels)}

    def replace_label(match):
        return f'{label_mapping[match.group(1)]}:'

    def replace_label_references(match):
        label = match.group(0)
        return label_mapping.get(label, label)

    # 레이블 정의를 변환
    code = label_pattern.sub(replace_label, code)
    
    # 레이블 참조를 변환
    for label, std_label in label_mapping.items():
        code = re.sub(r'\b' + re.escape(label) + r'\b', std_label, code)
    
    return code

# 전체 전처리 함수
def preprocess_assembly_code(code):
    code = remove_comments(code)
    code = remove_extra_whitespaces(code)
    code = standardize_code(code)
    code = standardize_labels(code)
    return code

# 디렉토리 내의 모든 어셈블리어 파일을 전처리하는 함수
def preprocess_files(directory):
    for root, dirs, files in os.walk(directory):
        for filename in files:
            if filename.endswith(".asm"):
                filepath = os.path.join(root, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as file:
                        code = file.read()
                    processed_code = preprocess_assembly_code(code)
                    
                    # Print the before and after states for debugging
                    if code != processed_code:
                        print(f"File {filepath} has been changed.")
                        print(f"Before:\n{code}\n")
                        print(f"After:\n{processed_code}\n")
                    
                    with open(filepath, 'w', encoding='utf-8') as file:
                        file.write(processed_code)
                    
                    # Print success message
                    print(f"Successfully processed {filepath}")
                except Exception as e:
                    print(f"Error processing file {filepath}: {e}")

# 어셈블리어 코드 파일들이 있는 디렉토리 경로 설정
directories = ['data_Set1', 'data_Set2', 'data_Set3', 'data_Set4']
for directory_path in directories:
    preprocess_files(directory_path)

```

### Epoch 1/10

- **Time**: 471 seconds
- **Training Accuracy**: 61.10%
- **Training Loss**: 0.6653
- **Validation Accuracy**: 81.50%
- **Validation Loss**: 0.5169

This indicates that the model is learning and improving significantly on the validation set.

### Epoch 2/10

- **Time**: 430 seconds
- **Training Accuracy**: 86.58%
- **Training Loss**: 0.4214
- **Validation Accuracy**: 87.25%
- **Validation Loss**: 0.2883

The model shows significant improvement in both training and validation accuracy, and a decrease in loss, suggesting better performance.

### Epoch 3/10

- **Time**: 409 seconds
- **Training Accuracy**: 95.89%
- **Training Loss**: 0.1595
- **Validation Accuracy**: 90.00%
- **Validation Loss**: 0.2152
